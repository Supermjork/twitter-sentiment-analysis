{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to work with\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gensim as gns\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the sake of Preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word Embedding\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the libraries' versions used in this notebook\n",
    "version_list = {\"NumPy Version:\": np.__version__,\n",
    "                \"Polars Version:\": pl.__version__,\n",
    "                \"MatPlotLib Version:\": mpl.__version__,\n",
    "                \"Seaborn Version:\": sns.__version__,\n",
    "                \"PyTorch Version:\": torch.__version__,\n",
    "                \"NLTK Version:\": nltk.__version__,\n",
    "                \"SpaCy Version:\": spacy.__version__,\n",
    "                \"Gensim Version:\": gns.__version__,\n",
    "                \"SciPy Version:\": sci.__version__}\n",
    "\n",
    "for (k, v) in version_list.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining path to install NLTK libraries in\n",
    "NLTK_LIB_PATH = \".\\\\venv_nlp\\\\Lib\\\\nltk_data\"\n",
    "\n",
    "# Defining download function\n",
    "def download_libs():\n",
    "    # Download extra parts of the library to use\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers\\\\punkt.zip\")                     # Punctuation\n",
    "        print(\"Punctuation Data Exists.\")\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', download_dir = NLTK_LIB_PATH)\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"corpora\\\\stopwords.zip\")                    # Stopwords\n",
    "        print(\"Stopwords Package Exists.\")\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', download_dir = NLTK_LIB_PATH)\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"corpora\\\\wordnet.zip\")                      # Corpus\n",
    "        print(\"Wordnet Package Exists.\")\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet', download_dir = NLTK_LIB_PATH)\n",
    "\n",
    "try:\n",
    "    os.mkdir(NLTK_LIB_PATH)\n",
    "\n",
    "    print(\"Directory for NLTK Packages Created.. Installing (Hopefully)\")\n",
    "    download_libs()\n",
    "except FileExistsError:\n",
    "    print(\"Directory Exists.\")\n",
    "    download_libs()\n",
    "except:\n",
    "    print(\"Couldn't Make Directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Importing our csv into our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "text_data = pl.read_csv(\"datasets/twitter_training.csv\", has_header=False, new_columns = [\"tweet_id\", \"entity\", \"sentiment\", \"tweet_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing first 10 rows\n",
    "text_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls\n",
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our Null count within the tweet content column is practically $<1\\%$ (to be exact $0.927\\%$), we can safely drop those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping nulls\n",
    "text_data = text_data.drop_nulls('tweet_content')\n",
    "text_data = text_data.drop('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the available rows and their information\n",
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Preprocessing the text so it's somewhat cleaner than when obtianed, so that the model doesn't struggle (Instead, we will :D )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stopword set\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add(\"im\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex function to remove special characters, links, etc.\n",
    "def regex_cleanse(text: str):\n",
    "    # URLS\n",
    "    text = re.sub(r'https\\S+', '', text)\n",
    "\n",
    "    # @<username>\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # #<word>\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # One character that doesn't belong to word or whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Attempt to remove linked pictures URLs\n",
    "    text = re.sub(r'pic\\w+', '', text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation Function\n",
    "def tokeniser(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading language model\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Lemmatiser\n",
    "def lemma(tokens):\n",
    "    doc = model(tokens)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing emojis\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text preprocessing function to apply to all rows\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    text = regex_cleanse(text.lower())\n",
    "    text = remove_emoji(text)\n",
    "    text = lemma(text)\n",
    "    #text = tokeniser(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cleaned-preprocessed dataset\n",
    "cleaned = text_data.with_columns(pl.col('tweet_content').map_elements(preprocess_text, return_dtype = list[str]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series of vectors from every sentence\n",
    "model_ready_text = cleaned['tweet_content'].to_list()\n",
    "\n",
    "model_ready_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Continuous Bag of Words, Word Embedding\n",
    "word2vec_cbow = Word2Vec(model_ready_text, min_count = 5, vector_size=100,  sg = 0, workers = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the vector for the word\n",
    "word2vec_cbow.wv['nvidia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Skip-gram Word Embedding\n",
    "word2vec_skgrm = Word2Vec(model_ready_text, min_count = 5, vector_size = 100, sg = 1, workers = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_skgrm.wv['nvidia']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
