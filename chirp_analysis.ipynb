{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to work with\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gensim as gns\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the sake of Preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word Embedding\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version: 1.26.4\n",
      "Polars Version: 1.12.0\n",
      "MatPlotLib Version: 3.9.2\n",
      "Seaborn Version: 0.13.2\n",
      "PyTorch Version: 2.5.1+cpu\n",
      "NLTK Version: 3.9.1\n",
      "SpaCy Version: 3.8.2\n",
      "Gensim Version: 4.3.3\n",
      "SciPy Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Display the libraries' versions used in this notebook\n",
    "version_list = {\"NumPy Version:\": np.__version__,\n",
    "                \"Polars Version:\": pl.__version__,\n",
    "                \"MatPlotLib Version:\": mpl.__version__,\n",
    "                \"Seaborn Version:\": sns.__version__,\n",
    "                \"PyTorch Version:\": torch.__version__,\n",
    "                \"NLTK Version:\": nltk.__version__,\n",
    "                \"SpaCy Version:\": spacy.__version__,\n",
    "                \"Gensim Version:\": gns.__version__,\n",
    "                \"SciPy Version:\": sci.__version__}\n",
    "\n",
    "for (k, v) in version_list.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Exists.\n",
      "Punctuation Data Exists.\n",
      "Stopwords Package Exists.\n",
      "Wordnet Package Exists.\n"
     ]
    }
   ],
   "source": [
    "# Defining path to install NLTK libraries in\n",
    "NLTK_LIB_PATH = \".\\\\venv_nlp\\\\Lib\\\\nltk_data\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(NLTK_LIB_PATH)\n",
    "\n",
    "    print(\"Directory for NLTK Packages Created.. Installing (Hopefully)\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory Exists.\")\n",
    "except:\n",
    "    print(\"Couldn't Make Directory.\")\n",
    "\n",
    "# Download extra parts of the library to use\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers\\\\punkt.zip\")                     # Punctuation\n",
    "    print(\"Punctuation Data Exists.\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt', download_dir = NLTK_LIB_PATH)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora\\\\stopwords.zip\")                    # Stopwords\n",
    "    print(\"Stopwords Package Exists.\")\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', download_dir = NLTK_LIB_PATH)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora\\\\wordnet.zip\")                      # Corpus\n",
    "    print(\"Wordnet Package Exists.\")\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', download_dir = NLTK_LIB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Importing our csv into our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "text_data = pl.read_csv(\"datasets/twitter_training.csv\", has_header=False, new_columns = [\"tweet_id\", \"entity\", \"sentiment\", \"tweet_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet_id</th><th>entity</th><th>sentiment</th><th>tweet_content</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;im getting on borderlands and â€¦</td></tr><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;I am coming to the borders andâ€¦</td></tr><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;im getting on borderlands and â€¦</td></tr><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;im coming on borderlands and iâ€¦</td></tr><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;im getting on borderlands 2 anâ€¦</td></tr><tr><td>2401</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;im getting into borderlands anâ€¦</td></tr><tr><td>2402</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;So I spent a few hours making â€¦</td></tr><tr><td>2402</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;So I spent a couple of hours dâ€¦</td></tr><tr><td>2402</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;So I spent a few hours doing sâ€¦</td></tr><tr><td>2402</td><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>&quot;So I spent a few hours making â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ tweet_id â”† entity      â”† sentiment â”† tweet_content                   â”‚\n",
       "â”‚ ---      â”† ---         â”† ---       â”† ---                             â”‚\n",
       "â”‚ i64      â”† str         â”† str       â”† str                             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† im getting on borderlands and â€¦ â”‚\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† I am coming to the borders andâ€¦ â”‚\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† im getting on borderlands and â€¦ â”‚\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† im coming on borderlands and iâ€¦ â”‚\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† im getting on borderlands 2 anâ€¦ â”‚\n",
       "â”‚ 2401     â”† Borderlands â”† Positive  â”† im getting into borderlands anâ€¦ â”‚\n",
       "â”‚ 2402     â”† Borderlands â”† Positive  â”† So I spent a few hours making â€¦ â”‚\n",
       "â”‚ 2402     â”† Borderlands â”† Positive  â”† So I spent a couple of hours dâ€¦ â”‚\n",
       "â”‚ 2402     â”† Borderlands â”† Positive  â”† So I spent a few hours doing sâ€¦ â”‚\n",
       "â”‚ 2402     â”† Borderlands â”† Positive  â”† So I spent a few hours making â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing first 10 rows\n",
    "text_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>tweet_id</th><th>entity</th><th>sentiment</th><th>tweet_content</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>74682.0</td><td>&quot;74682&quot;</td><td>&quot;74682&quot;</td><td>&quot;73996&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;686&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>6432.586165</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>3740.42787</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>&quot;Amazon&quot;</td><td>&quot;Irrelevant&quot;</td><td>&quot; &quot;</td></tr><tr><td>&quot;25%&quot;</td><td>3195.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>6422.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>9601.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>13200.0</td><td>&quot;johnson&amp;johnson&quot;</td><td>&quot;Positive&quot;</td><td>&quot;ğŸ§» at Home Depot on Hanley... Iâ€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 5)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† tweet_id    â”† entity          â”† sentiment  â”† tweet_content                   â”‚\n",
       "â”‚ ---        â”† ---         â”† ---             â”† ---        â”† ---                             â”‚\n",
       "â”‚ str        â”† f64         â”† str             â”† str        â”† str                             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 74682.0     â”† 74682           â”† 74682      â”† 73996                           â”‚\n",
       "â”‚ null_count â”† 0.0         â”† 0               â”† 0          â”† 686                             â”‚\n",
       "â”‚ mean       â”† 6432.586165 â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ std        â”† 3740.42787  â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ min        â”† 1.0         â”† Amazon          â”† Irrelevant â”†                                 â”‚\n",
       "â”‚ 25%        â”† 3195.0      â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ 50%        â”† 6422.0      â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ 75%        â”† 9601.0      â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ max        â”† 13200.0     â”† johnson&johnson â”† Positive   â”† ğŸ§» at Home Depot on Hanley...   â”‚\n",
       "â”‚            â”†             â”†                 â”†            â”† Iâ€¦                              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for nulls\n",
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our Null count within the tweet content column is practically $<1\\%$ (to be exact $0.927\\%$), we can safely drop those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = text_data.drop_nulls('tweet_content')\n",
    "text_data = text_data.drop('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>entity</th><th>sentiment</th><th>tweet_content</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;73996&quot;</td><td>&quot;73996&quot;</td><td>&quot;73996&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;Amazon&quot;</td><td>&quot;Irrelevant&quot;</td><td>&quot; &quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;johnson&amp;johnson&quot;</td><td>&quot;Positive&quot;</td><td>&quot;ğŸ§» at Home Depot on Hanley... Iâ€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† entity          â”† sentiment  â”† tweet_content                   â”‚\n",
       "â”‚ ---        â”† ---             â”† ---        â”† ---                             â”‚\n",
       "â”‚ str        â”† str             â”† str        â”† str                             â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 73996           â”† 73996      â”† 73996                           â”‚\n",
       "â”‚ null_count â”† 0               â”† 0          â”† 0                               â”‚\n",
       "â”‚ mean       â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ std        â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ min        â”† Amazon          â”† Irrelevant â”†                                 â”‚\n",
       "â”‚ 25%        â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ 50%        â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ 75%        â”† null            â”† null       â”† null                            â”‚\n",
       "â”‚ max        â”† johnson&johnson â”† Positive   â”† ğŸ§» at Home Depot on Hanley...   â”‚\n",
       "â”‚            â”†                 â”†            â”† Iâ€¦                              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Preprocessing the text so it's somewhat cleaner than when obtianed, so that the model doesn't struggle (Instead, we will :D )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stopword set\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add(\"im\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex function to remove special characters, links, etc.\n",
    "def regex_cleanse(text: str):\n",
    "    text = re.sub(r'https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'pic\\w+', '', text)\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemma(tokens):\n",
    "    doc = model(tokens)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text preprocessing function to apply to all rows\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    text = regex_cleanse(text.lower())\n",
    "    text = remove_emoji(text)\n",
    "    text = lemma(text)\n",
    "    #text = tokeniser(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chungus', 'bungus', 'ungus']\n"
     ]
    }
   ],
   "source": [
    "txt = preprocess_text(\"we chungus bungus ungus in this\")\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = text_data.with_columns(pl.col('tweet_content').map_elements(preprocess_text, return_dtype = list[str]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>entity</th><th>sentiment</th><th>tweet_content</th></tr><tr><td>str</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>[&quot;get&quot;, &quot;borderland&quot;, &quot;murder&quot;]</td></tr><tr><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>[&quot;come&quot;, &quot;border&quot;, &quot;kill&quot;]</td></tr><tr><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>[&quot;get&quot;, &quot;borderland&quot;, &quot;kill&quot;]</td></tr><tr><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>[&quot;come&quot;, &quot;borderland&quot;, &quot;murder&quot;]</td></tr><tr><td>&quot;Borderlands&quot;</td><td>&quot;Positive&quot;</td><td>[&quot;get&quot;, &quot;borderland&quot;, â€¦ &quot;murder&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ entity      â”† sentiment â”† tweet_content                   â”‚\n",
       "â”‚ ---         â”† ---       â”† ---                             â”‚\n",
       "â”‚ str         â”† str       â”† list[str]                       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Borderlands â”† Positive  â”† [\"get\", \"borderland\", \"murder\"â€¦ â”‚\n",
       "â”‚ Borderlands â”† Positive  â”† [\"come\", \"border\", \"kill\"]      â”‚\n",
       "â”‚ Borderlands â”† Positive  â”† [\"get\", \"borderland\", \"kill\"]   â”‚\n",
       "â”‚ Borderlands â”† Positive  â”† [\"come\", \"borderland\", \"murderâ€¦ â”‚\n",
       "â”‚ Borderlands â”† Positive  â”† [\"get\", \"borderland\", â€¦ \"murdeâ€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>entity</th><th>sentiment</th><th>tweet_content</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;73996&quot;</td><td>&quot;73996&quot;</td><td>73996.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;Amazon&quot;</td><td>&quot;Irrelevant&quot;</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;johnson&amp;johnson&quot;</td><td>&quot;Positive&quot;</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† entity          â”† sentiment  â”† tweet_content â”‚\n",
       "â”‚ ---        â”† ---             â”† ---        â”† ---           â”‚\n",
       "â”‚ str        â”† str             â”† str        â”† f64           â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 73996           â”† 73996      â”† 73996.0       â”‚\n",
       "â”‚ null_count â”† 0               â”† 0          â”† 0.0           â”‚\n",
       "â”‚ mean       â”† null            â”† null       â”† null          â”‚\n",
       "â”‚ std        â”† null            â”† null       â”† null          â”‚\n",
       "â”‚ min        â”† Amazon          â”† Irrelevant â”† null          â”‚\n",
       "â”‚ 25%        â”† null            â”† null       â”† null          â”‚\n",
       "â”‚ 50%        â”† null            â”† null       â”† null          â”‚\n",
       "â”‚ 75%        â”† null            â”† null       â”† null          â”‚\n",
       "â”‚ max        â”† johnson&johnson â”† Positive   â”† null          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'borderland', 'murder']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a series of vectors from every sentence\n",
    "model_ready_text = cleaned['tweet_content'].to_list()\n",
    "\n",
    "model_ready_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cbow = Word2Vec(model_ready_text, min_count = 5, vector_size=100,  sg = 0, workers = 10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.38710666e-01,  7.39726305e-01,  6.61083534e-02,  9.77369606e-01,\n",
       "        9.96545702e-02,  5.15600443e-01,  1.37819541e+00, -9.09821391e-02,\n",
       "       -1.66094720e+00, -1.03679037e+00,  6.54140264e-02, -7.50853598e-01,\n",
       "       -9.22983348e-01, -2.31775641e-02, -6.88205183e-01, -1.31816622e-02,\n",
       "        1.70911455e+00, -4.06895757e-01,  3.09767067e-01,  5.42064190e-01,\n",
       "        7.13509619e-01, -4.74175304e-01,  4.48252857e-02,  1.29425216e+00,\n",
       "       -7.17418909e-01,  6.14135563e-01, -1.75333932e-01, -1.42536151e+00,\n",
       "        2.20917404e-01, -9.72289324e-01, -5.76742113e-01,  4.19251382e-01,\n",
       "        7.89716287e-05, -4.14689034e-01,  2.23332429e+00,  2.06944799e+00,\n",
       "        4.34534520e-01,  7.12677717e-01, -1.13264954e+00, -2.44789794e-01,\n",
       "       -1.45503968e-01, -1.15199745e+00, -1.95202899e+00,  1.17559755e+00,\n",
       "       -1.02075088e+00, -7.86058009e-02,  1.25684333e+00, -5.90531349e-01,\n",
       "       -8.50952923e-01, -1.66625902e-02,  4.12924260e-01,  2.62208152e+00,\n",
       "       -9.39794540e-01,  9.27228391e-01, -7.85121381e-01,  4.24949646e-01,\n",
       "        1.41669452e+00,  5.25522172e-01, -2.76501513e+00,  6.04113825e-02,\n",
       "        1.14560135e-01,  3.48559588e-01, -9.91039634e-01,  1.36841202e+00,\n",
       "        6.23531163e-01, -1.50170967e-01, -4.60139066e-01, -4.02622312e-01,\n",
       "        2.33176023e-01, -7.72503838e-02,  1.56503677e-01,  5.00597477e-01,\n",
       "        1.45453882e+00, -1.53003074e-02,  2.22683996e-01,  2.27444124e+00,\n",
       "       -1.18192208e+00,  1.59335947e+00,  5.41793644e-01,  4.08787966e-01,\n",
       "       -1.12355852e+00,  5.31904042e-01, -2.50754319e-02, -5.57485342e-01,\n",
       "        8.55610669e-02,  7.02543616e-01,  5.00722647e-01,  1.62737370e+00,\n",
       "        1.26632667e+00,  1.16908419e+00,  1.45895612e+00, -8.55826139e-01,\n",
       "        6.17379010e-01,  1.46694040e+00,  1.03651561e-01,  2.08671308e+00,\n",
       "       -1.46802795e+00,  1.06552100e+00, -3.12309444e-01, -4.60953712e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cbow.wv['nvidia']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
